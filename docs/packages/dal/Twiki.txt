<!-- /ActionTrackerPlugin -->
<LINK href="/twiki/pub/TWiki/KupuContrib/kuputwiki.css" type=text/css
rel=stylesheet>
<!-- /ActionTrackerPlugin --><LINK
href="/twiki/pub/TWiki/KupuContrib/kuputwiki.css" type=text/css
rel=stylesheet>
<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace
%TOPIC% below with i.e "My Topic"!-->
<!---------------------------------------------------------  snip snip
----------------------------------------------------------------->
%CERTIFY% 
---+!! <nop>%TOPIC% 
%TOC% <!--optional-->%STARTINCLUDE% 

JCF, Jan-23-2023: the following is the original ATLAS TDAQ Twiki; it is not guaranteed to be applicable to the DUNE DAQ refactor of this repository


The DAL (Data Access Library) is a software package to simplify access to the
configuration data created on top of the *core* schema. It is automatically
generated by the *dal* package for C++, Java and Python programming languages.
To use the DAL package you need to read the
[[https://pcatd12.cern.ch/doxygen/tdaq-02-00-00/html/ConfigPackages.html
][config User's Guide]]. 

---+ 1. Responsible 

%ICON{person}% [[http://consult.cern.ch/xwho/people/432778][Igor Soloviev]]

---+ 2. Documentation 

Below there are several useful links pointing to release-specific
documentation.<!-- Add an introduction here, describing the purpose of this
topic. !--> 

---++ 2.1. Release tdaq-01-09-01

%Y% The release tdaq-01-09-01 is production release. 

Available documentation: 
   * <div class="TML">the core schema</div>
      * <div class="TML">the
      * [[http://pcatd12.cern.ch/releases/tdaq-01-09-01/installed/share/data/daq/schema/core.schema.xml][OKS
      * xml]] file</div>
      * <div class="TML">several
      * [[http://pcatd12.cern.ch/releases/tdaq-01-09-01/installed/share/doc/dal/views/][views
      * (eps, mif, xml)]]&nbsp;created using OKS Schema Editor for sub-sets of
      * the core schema classes</div>
      * <div class="TML">generated
      * [[http://pcatd12.cern.ch/releases/tdaq-01-09-01/installed/share/doc/DAQRelease/html/core.schema.xml][XSLT]]
      * description for all classes</div>
   * <div class="TML">the
   * [[http://pcatd12.cern.ch/doxygen/tdaq-01-09-01/html/namespacedaq_1_1core.html][C++
   * doxygen]] generated documentation</div>
   * <div class="TML">the
   * [[http://pcatd12.cern.ch/javadoc/tdaq-01-09-01/dal/package-summary.html][JavaDoc]]
   * generated documentation</div>

---++ 2.2. Release tdaq-02-00-00

%Y% The release tdaq-02-00-00 is new release. 

Available documentation: 
   * <div class="TML">the core schema</div>
      * <div class="TML">the
      * [[http://pcatd12.cern.ch/releases/tdaq-02-00-00/installed/share/data/daq/schema/core.schema.xml][OKS
      * xml]] file</div>
      * <div class="TML">several
      * [[http://pcatd12.cern.ch/releases/tdaq-02-00-00/installed/share/doc/dal/views/][views
      * (eps, mif, xml)]]&nbsp;created using OKS Schema Editor for sub-sets of
      * the core schema classes</div>
      * <div class="TML">generated
      * [[http://pcatd12.cern.ch/releases/tdaq-02-00-00/installed/share/doc/DAQRelease/html/core.schema.xml][XSLT]]
      * description for all classes</div>
   * <div class="TML">the
   * [[http://pcatd12.cern.ch/doxygen/tdaq-02-00-00/html/namespacedaq_1_1core.html][C++
   * doxygen]] generated documentation</div>
   * <div class="TML">the
   * [[http://pcatd12.cern.ch/javadoc/tdaq-02-00-00/dal/package-summary.html][JavaDoc]]
   * generated documentation</div>

---++ 2.3. Release nightly 

%X% The availability of documentation for nightly release depends on the
%[[http://pcatd12.cern.ch/cmt/releases/nightly.html][status]] of the nightly
%build. If the build fails, it may be unavailable. In such case contact the
%software librarian. 

Available documentation: 
   * <div class="TML">the core schema</div>
      * <div class="TML">the
      * [[http://pcatd12.cern.ch/releases/nightly/installed/share/data/daq/schema/core.schema.xml][OKS
      * xml]] file</div>
      * <div class="TML">several
      * [[http://pcatd12.cern.ch/releases/nightly/installed/share/doc/dal/views/][views
      * (eps, mif, xml)]]&nbsp;created using OKS Schema Editor for sub-sets of
      * the core schema classes</div>
      * <div class="TML">generated
      * [[http://pcatd12.cern.ch/releases/nightly/installed/share/doc/DAQRelease/html/core.schema.xml][XSLT]]
      * description for all classes</div>
   * <div class="TML">the
   * [[http://pcatd12.cern.ch/doxygen/nightly/html/namespacedaq_1_1core.html][C++
   * doxygen]] generated documentation</div>
   * <div class="TML">the
   * [[http://pcatd12.cern.ch/javadoc/nightly/dal/package-summary.html][JavaDoc]]
   * generated documentation</div>

---+ 3. Basic Concepts

Below there is description of several main classes, algorithms and utilities
provided by the DAL package. 

---++ 3.1. Partition Class

A partition is a sub-set of the ATLAS systems and detectors for the purpose of
data taking. In the configuration database it is described by an object of the
*Partition* class. The participating systems and detectors are described as
objects of the *Segment* class, linked with partition object.

<div style="text-align: center;"> %ATTACHURL%/Partition.gif </div>

---++ 3.2. Segment Classes

A segment is self-sufficient part of the system which can be configured and
controlled independently from the rest of the TDAQ system. A segment
represents a detector, a system or their part. A segment can include other
segments. For example: 
   * <div class="TML">the Tile detector segment includes tile EBA, EBC, LBA,
   * LBC and monitoring segments; the Tile segment or any it's sub-segments
   * can be included into partition individually</div>
   * <div class="TML">the partition includes ROS, EB, LVL2, EF TDAQ and
   * several detector segments</div>

A segment is controlled by associated RC controller application. 

In the configuration database a segment is described by an object of the
*Segment* class. To be used by the partition it has to be added to the
partition's _Segments_ relationship. To be temporary ignored or taken out
(e.g. for tests, fixing problems, etc.) a segment can be disabled, i.e. added
to the partition's _Disabled_ relationship. When a segment is disabled, the
DAL algorithms consider all nested segments also disabled. The nested segments
are linked with parent segment via _Segments_ relationship.

<div style="text-align: center;"> %ATTACHURL%/Segment1.gif </div>

In addition to nested segments, a segment can include applications and
resources. The resources are described by objects of classes derived from the
*ResourceBase* class and the applications are described by objects of classes
derived from the *BaseApplication* class. The resources are linked with
segment via _Resources_ relationship. There are several types of applications
and depending of it's type an application can be linked with segment via
different relationships (see next section). 

<div style="text-align: center;"> %ATTACHURL%/Segment2.gif </div>

---++ 3.3. Application Classes

An application object corresponds to a process or several processes to be
started under certain conditions. An application is described a class derived
from the *BaseApplication* class. There are two main sub-types of
applications: 
   * the _normal_ applications, which specify computer where to run the
   * process (the relationship is empty for _localhost_ partitions) and
   * correspond to one-and-only-one process to be started; such applications
   * are described by an object of the *Application* class or an object of
   * class derived from it; a normal application object belongs to
   * one-and-only-one segment;
   * the _template_ applications, which specify set of computers where similar
   * processes have to started; such applications are described by an object
   * of the *TemplateApplication* class or an object of class derived from it;
   * a template application object can be shared between several segments;
   * more information about template applications can be found in the
   * [[http://edms.cern.ch/document/684859][Template Applications Proposal]]
   * document.

<div style="text-align: center;"> %ATTACHURL%/Applications.gif </div>

The normal applications can be linked with Segment by one of the following
ways: 
   * via _Applications_ relationship (typical user application);
   * via _Resources_ relationship (when application is a resource, see
   * Resources section for more information)
   * via _Infrastructure_ relationship (vital segment's infrastructure
   * applications; to be run before any other applications in this segment;
   * have associated list of backup computes to be restarted on one of them in
   * case of problems with default one)

<div style="text-align: center;"> %ATTACHURL%/ApplicationsOfSegment.gif </div>

The template applications can be linked with special template segment (an
object of *HLTSegment* or derived class) via _TemplateApplications_
relationship. 

The segment's controller application is linked with it's segment via
_IsControlledBy_ relationship. The controller can be a normal application or
template application. In the latter case the segment's _DefaultHost_
relationship has to point to an object of the Computer class (except the
_localhost_ partitions). 

<img width="16" alt="" src="%PUBURL%/TWiki/TWikiDocGraphics/tip.gif"
height="16" classname="undefined">&nbsp;An application has to be linked with
segment only using one relationship. For example, a resource application
object (i.e. when it's class has two base classes: the !ResourceBase and the
Application) has to be linked with segment object only via _Resources_
relationship; it shall not be added to the _Applications_ relationship at the
same time. 

---+++ 3.3.1. Application Names

To be uniquely identified in the system, each process created from the
application object has unique name. Such names are used by applications to
publish information about themselves in
[[http://atlas-tdaq-monitoring.web.cern.ch/atlas-tdaq-monitoring/IS/Welcome.htm][IS]],
to create log or data files, etc. For normal applications the unique name is
equal to the application database object unique ID. For template applications
such name is calculated dynamically. It was made as short as possible without
losing useful information allowing identifying the application. The format of
template application names is the following:
   * the "appID:segmentID:short-hostID:instance" is used for most template
   * applications, e.g. "PT:EFarm10:pc-tdq-ef-22:3"
   * the "DcName-DcAppInstanceId" is used for Level-2 data-collection
   * applications, e.g. "L2PU-7654"
   * the "segmentID:short-hostID" is used for template controllers (one per
   * host and per segment), e.g. "EFarm10:pc-tdq-ef-22"

%X% Since usage of the message passing instance ID in the level-2 application
%name is the mandatory requirement of the level-2 community, the mapping
%between level-2 application name and corresponding configuration database
%object, a segment it belongs and a host where it is running is not trivial.
%The calculation of the data collection instance ID requires knowledge of the
%message passing node ID, that in turn requires iteration through all
%segments, resources and applications defined in the partition to find all
%level-2 template applications. That in addition makes such mapping an
%expensive operation from database point of view. One of the ways to know such
%mapping is the usage of the [[#DalDumpAppConfigUtility][dal_dump_app_config]]
%binary (see below for more info).

---++ 3.4. Resource Classes

A _simple resource_ is the smallest part of the ATLAS TDAQ system which can be
individually disabled (i.e. masked out of the ATLAS TDAQ), and possibly
enabled, without stopping the data taking process.

A _resource set_ consists of simple resources or another resource sets and it
is used to describe closely-coupled resources, where disabling/enabling of
single resource may affect a necessity to disable/enable others. Disabling of
the resource set means disabling of all resources belonging to it.

In the configuration database a simple resource is described by an object of
the *Resource* class and a resource set is described by the *ResourceSet*
class. To be used by the segment it has to be added to the segment's
_Resources_ relationship. Like in case with segments, to be temporary ignored
or taken out (e.g. for tests, fixing problems, etc.) a resource can be
disabled, i.e. added to the partition's _Disabled_ relationship. When a
resource set is disabled, the DAL algorithms consider all nested resources
also disabled. The nested resource objects are linked with parent resource set
via _Contains_ relationship. 

There are two special types of the resource set: the _resource-set-or_ and
_resource-set-and_, which are described by objects of the *ResourceSetOr* and
the *ResourceSetAnd* configuration database class. The DAL algorithms consider
objects of those classes automatically disabled, when respectively any or all
nested resources of that resource sets are disabled. For more information
about these classes see the [[http://edms.cern.ch/document/812273][Cabling
Description Schema Extension Proposal]] document and
[[TDAQConfigSchemaExtension][relevant discussion with detectors]].

<div style="text-align: center;"> %ATTACHURL%/Resources.gif </div>

---++ 3.5. Software Repository Classes and Utilities

An application configuration object is linked with _software-object_ (i.e.
_computer-program_ or _script_ object created from related *Binary* or
*Script* database class) belonging to a software repository. A software
repository object (i.e. an object of the *SW_Repository* class) describes
common properties for all software objects it contains. In particular such
properties are:
   * installation paths to the software objects it contains (i.e. binaries,
   * scripts, libraries and jar files)
   * common process environment to be set for above software objects
   * the IS-info files describing IS data created by above software objects
   * the IGUI panels dealing with information or status of above software
   * objects
   * other used software repositories

A software repository is considered to be _used_ in the scope of partition, if
there is an application using it and belonging to a segment of this partition.
The disabled segments and resource applications are also taken into account.

A software repository corresponds to the software releases built using CMT.
This allows assuming standard paths for installation of the computer programs,
scripts and libraries. There is special *SW_ExternalPackage* class which is
used for third-party software releases and packages, where mapping between
names of directories for installed libraries and standard CMT tags is required
(for more information see the [[http://edms.cern.ch/document/732735][Proposals
for Description of External Packages]] document).

<div style="text-align: center;"> %ATTACHURL%/SW_Repository.gif </div>

A software repository description can be created manually using OKS tools, or
can be generated by the DAL utility _dal_create_sw_repository_. The latter one
is integrated into TDAQ release build and also used some detectors. Below
there are more details about software repository properties and their
generation.

---+++ 3.5.1. Generation of Software Objects

To add computer programs and scripts into software repository one has to put
few macros into cmt/requirements file of package, which builds or installs
them. The syntax of the macros is the following:

<verbatim>
    macro sw.repository.binary.${NAME}:${property}     "value"
    macro sw.repository.script.${NAME}:${property}     "value"
    macro sw.repository.java.${NAME}.jar:${property}   "value"
</verbatim>

The ${NAME} defines the object ID of the generated software object. If the
binary.name property is not set explicitly, then it has to be equal to the
computer program or script name (e.g. ipc_server, rdb_server, igui_start,
etc.) or jar file name.

The allowed properties are:
| *Property* | *Value* |
| name  | Short description.  |
| binary.name  | Explicitly define script or binary name.  |
| description  | Explicitly provide software object name; if not defined, is
extracted from script or binary running with _--help_ command line option.  |
| help.url  | Set help URL describing software object.  |
| default.parameters  | Set default parameters for script or binary.  |
| needs.resource  | Set RM resources used by the script or binary. Can be
repeated several times to set multiple resources.  |
| needs.environment  | Set process environment required by the script or
binary. Can be repeated several times to set multiple environment variables.
|
| includes  | Include database files containing configuration objects required
for description of this software object. Can be repeated several times to
include several files.  |
| uses  | Set used software repositories and external packages. Can be
repeated several times to set usage of several objects.  |

To be generated, at least one above macro describing software object has to be
put into requirements file.

An example is shown below:

<pre>
      # <i>generate description of binary object loading IS info</i>
    macro <b>sw.repository.binary.</b>dal_load_is_info_files:<b>name</b>
"load IS info files on rdb server"

      # <i>generate description for dal.jar file</i>
    macro <b>sw.repository.java.</b>dal.jar:<b>name</b>           "core dal"
    macro <b>sw.repository.java.</b>dal.jar:<b>description</b>    "generated
dal for classes defined by the core.schema.xml"
    macro <b>sw.repository.java.</b>dal.jar:<b>help.url</b>
"https://twiki.cern.ch/twiki/bin/save/Atlas/DaqHltDal"
</pre>

---+++ 3.5.2. Generation of External Packages and Variables

To generate description of the external software packages one has to put
macros using the following syntax:

<verbatim>
    macro sw.external.package.${NAME}:${property}     "value"
</verbatim>

The ${NAME} defines the object ID of the generated package object. The allowed
properties and meaning of their values are:

| *Property* | *Value* |
| description  | Description of the external package.  |
| installation.path  | The package's top-level directory, i.e. the root
directory of all package' subdirectories.  |
| cmt.tag  | The CMT tag, for which current description is generated.  |
| lib.mapping  | Mapping of the path to installed libraries of this package
for given CMT tag. The patch is relative to the intallation path.  |
| bin.mapping  | Mapping of the path to installed binaries of this package for
given CMT tag. The patch is relative to the intallation path.  |
| uses  | Set used external packages. Can be repeated several times to set
usage of several objects.  |
| needs.environment  | Set process environment required by the script or
binary. Can be repeated several times to set multiple environment variables.
|

To be generated, at least the _installation.path_, the _cmt.tag_ and
_lib.mapping_ macros describing software package have to be put into
requirements file.

An example for package CORAL for i686-slc4-gcc34-opt CMT tag is shown below:
<pre>
    macro <b>sw.external.package</b>.CORAL:<b>installation.path</b>
'${CORAL_HOME}'
    macro <b>sw.external.package</b>.CORAL:<b>cmt.tag</b>
i686-slc4-gcc34-opt
    macro <b>sw.external.package</b>.CORAL:<b>lib.mapping</b>
slc4_ia32_gcc34/lib
    macro <b>sw.external.package</b>.CORAL:<b>bin.mapping</b>
slc4_ia32_gcc34/bin
</pre>

%ICON{note}% The above example uses a variable. The generation of
%package-specific variables normally should be done at the same moment as
%generation of the package, and the description of variables should be stored
%in the same xml file, as description of their packages.

To generate description of the external software packages one has to put
macros using the following syntax:

<verbatim>
    sw.environment.variable.${NAME}:${property}     "value"
</verbatim>

The ${NAME} defines the object ID of the generated package object, which is
equal to the variable name. The allowed properties and meaning of their values
are:

| *Property* | *Value* |
| value  | Value of the variable.  |
| description  | Description of the variable.  |

The _value_ macro is mandatory to generate description of a variable object.

An example of variables for package CORAL is shown below:

<pre>
    macro <b>sw.environment.variable</b>.LCG_INST_PATH:<b>value</b>
/afs/cern.ch/sw/lcg
    macro <b>sw.environment.variable</b>.CORAL_VERSION:<b>value</b>
CORAL_1_9_0
    macro <b>sw.environment.variable</b>.CORAL_HOME:<b>value</b>
${LCG_INST_PATH}/app/releases/CORAL/${CORAL_VERSION}
</pre>

%ICON{more}% The generation of the description of external packages and their
%variables should be done for each supported CMT tag. The
%_dal_create_sw_repository_ generation utility should be run for each
%supported tag using the xml file generated on previous step, e.g.:
<pre>
      # <i><b>create</b> description for i686-slc4-gcc34-opt</i>
    dal_create_sw_repository -r i686-slc4-gcc34-opt.external.macros -o
externals.data.xml -e SEAL CORAL ...
      # <i><b>add</b> description for i686-slc4-gcc34-dbg</i>
    dal_create_sw_repository -r i686-slc4-gcc34-dbg.external.macros -o
externals.data.xml -e SEAL CORAL ...
      # <i><b>add</b> description for i686-slc3-gcc323-opt</i>
    dal_create_sw_repository -r i686-slc3-gcc323-opt.external.macros -o
externals.data.xml -e SEAL CORAL ...
</pre>
Above example will generate description of SEAL, CORAL, etc. packages for 3
platforms. Each sw package may have own mapping of directories containing
libraries and binaries files and values of variables depending on platform.
The core schema and the generation utility support the multi-value environment
variables, which value depend on platform, e.g. environment variable
SEAL_PLUGINS has value dependent on CMT tag:
<pre>
    ${COOL_HOME}/<b>slc4_ia32_gcc34</b>/lib/modules:${SEAL_HOME}/<b>slc4_ia32_gcc34</b>/lib/modules:...
# <i>on i686-slc4-gcc34-xxx</i>
    ${COOL_HOME}/<b>slc3_ia32_gcc323</b>/lib/modules:${SEAL_HOME}/<b>slc3_ia32_gcc323</b>/lib/modules:...
# <i>on i686-slc3-gcc323-opt</i>
</pre>

---+++ 3.5.3. Generation and Calculation of Used IS Info Files

The IS info schema files contain description of data created by the binaries
belonging to the software repository. Such files have to be loaded in special
rdb_server to make such descriptions available online to some IS applications.
The software repository object stores names of such files in the
_ISInfoDescriptionFiles_ multi-string attribute (the names are relative to the
repository' installation path). The list of such files is calculated by the
_dal_load_is_info_files_ utility, reading the configuration database and
checking all software repositories using by given partition.

To be automatically generated by the dal_create_sw_repository utility, the IS
info files have to be described using the following macros put into
appropriate CMT requirements files:

The syntax:
<verbatim>
    macro sw.repository.is-info-file.${FILE}:name "description"
</verbatim>

Above the FILE value is the filename relative to the repository installation
path.

The example below shows how to add the file
${TDAQ_INST_PATH}/share/data/oks2coral/oks-archive-info.xml to the list of
used IS info files:

<pre>
    macro
<b>sw.repository.is-info-file.</b>share/data/oks2coral/oks-archive-info.xml:<b>name</b>
"OKS Archive Info"
</pre>

---+++ 3.5.4. Generation and Calculation of IGUI Properties and Required Java
Jar Files

When IGUI starts, it has to load user-defined panels or in more wide scope to
apply user-specific properties. The calculation of complete list of such
properties and associated java jar files is implemented via configuration
database. The user-defined properties and required jar files are associated
with the software repositories. They are taken into account for any software
repository used by given partition. The list of such properties is calculated
by the _dal_get_igui_setup_ utility, reading the configuration database and
checking all software repositories using by given partition. This utility is
internally used by IGUI start script. The properties are stored by the sw
repository _IGUIProperties_ multi-value string attribute.

To be automatically generated by the dal_create_sw_repository utility, the
IGUI properties have to be described in the CMT requirements files defined
using the following syntax:

<verbatim>
    sw.repository.igui-properties.${NAME}: "java property"
</verbatim>

The NAME value is an arbitrary property name. It is reserved for future usage,
for example:

<pre>
    macro <b>sw.repository.igui-properties</b>.logfile:
"-Digui.logfile=${TDAQ_LOGS_PATH}/igui.out"
    macro <b>sw.repository.igui-properties</b>.L1CaloPanel:
"-Digui.panel=l1calo.L1CaloPanel "
</pre>

The IGUI Java property value should have a format -D${property}=${value}. For
multiple entries of properties with the same name, the values are put into
colon-separated list. Such technique is used for IGUI panels. For example see
value of _IGUI_PROPERTIES_ variable from below example.
The Java CLASSPATH is concatenated from jar files of all used software
repositories. See example below for M4 combined partition:

<pre>
    bash$ dal_get_igui_setup -d
oksconfig:combined/partitions/m4_combined.data.xml -p m4_combined -s sh
    export <b>__IGUI_PROPERTIES__</b>="  \
        -Digui.ed=TileCaledPanel -Digui.logfile=/logs/M4/m4_combined/igui.out
\
        -Digui.panel=TGCI_ParametersPanel:TGCI_StatusPanel:TileCalIguiPanel:igui.SctSupervisorPanel:l1calo.L1CaloPanel
\
        -Dl1calo.root=/det/l1calo/releases/pro/installed
-Donline.isServer.name=LargParams \
        -Donline.panel.name=LargOnlinePanel -Donline.root.segment=LArg"
    export
<b>__IGUI_CLASSPATH__</b>="/det/l1calo/releases/l1calo-00-05-15/installed/share/lib/l1calo.jar:\
        /det/l1calo/releases/l1calo-00-05-15/installed/share/lib/l1calo_dal.jar:\
        /det/l1calo/releases/l1calo-00-05-15/installed/share/lib/l1calo_ed.jar:\
        ...
        /det/muon/sw/installed//share/lib/TGCPanel.jar"
</pre>

---+++ 3.5.5. Installation Path Variables

In many cases it is usefull to have a
[[#4_1_3_Converter_for_substitution][substitution parameter]] pointing to the
software repository installation area and a process environment variable with
the same value. Such variables can be created automatically by DAL algorithms.
To do this the sw repository' _InstallationPathVariableName_ has to be filled
by a variable name. For all applications using such software repository the
relevant process environment variable will be created on fly. As well, the
similar substitution parameter will be created at scope of partition.

For example for M4 combined partition:
   * object "L1Calo@SW_Repository"
      * attribute
      * !InstallationPath="/det/l1calo/releases/l1calo-00-05-15/installed"
      * attribute !InstallationPathVariableName="L1CALO_INST_PATH"
      * %ICON{hand}% result automatic generation of
      * !L1CALO_INST_PATH="/det/l1calo/releases/l1calo-00-05-15/installed"
      * variable for all !L1Calo application
   * object "Online@SW_Repository"
      * attribute !InstallationPath="/sw/atlas/tdaq/tdaq-01-08-01/installed"
      * attribute !InstallationPathVariableName="TDAQ_INST_PATH"
      * %ICON{hand}% result automatic generation of
      * TDAQ_INST_PATH="/sw/atlas/tdaq/tdaq-01-08-01/installed" variable for
      * all applications (_all_ since any one is using TDAQ)

---+++ 3.5.6. Extendable Software Package Variables

The SW_PackageVariable class describes expendable environment variables
defined by the software packages (links the variables via new
!AddProcessEnvironment relationship). The class defines the variable name and
the suffix concatenated with the installation path of sw package it is linked
with. When the same package variable is linked with several packages, their
values are concatenated with colon (i.e. ':') separator. The values defined
via such mechanism are prefixed to a normal variable value, if it is defined.

---+++ 3.5.7. Generation of Segment-Wide Process Environment by Infrastructure
Applications

An infrastructure application server may provide service used by all
applications running inside this segment (e.g. rdb_server, is_server,
dbproxy). It is necessary to pass the name of the server to all it's clients.
This process can be automated using process environment variables generated by
DAL algorithm using values of attributes !SegmentProcEnvVarName,
!SegmentProcEnvVarParentName and !SegmentProcEnvVarValue of the
!InfrastructureApplication class.

If value of the !SegmentProcEnvVarName attribute is non-empty, the process
environment variable with name equal to value of this attribute is created for
any application of the segment the infrastructure application belongs to (use
case: pass name of server to all clients in the segment). The value of this
variable is calculated depending on the value of the !SegmentProcEnvVarValue
attribute: it either can be the application ID, or the name of the host the
infrastructure application runs on.

If value of the !SegmentProcEnvVarParentName attribute is not empty, the
process environment variable with name equal to the attribute's value is
generated for all applications of the segment. The value of the environment is
equal to value set for variable corresponding to the !SegmentProcEnvVarName
from a parent segment (use case: pass name of top-level server to the server
of this segment, e.g. from intermediate dbproxy to it's child).

---++++!! Example: segment rdb_server

   1. set rdb_server's !SegmentProcEnvVarName=TDAQ_DB_NAME and
!SegmentProcEnvVarValue=appId
   1. run rdb server with option "-a TDAQ_DB_NAME"; %BR%
   %H% the "-a" option says: take name from environment with name
   %TDAQ_DB_NAME; %BR%
   %X% do not use -d XYZ command line option and do not create any process
   %environment to pass name of rdb_server to segment's applications as it was
   %before!
   1. as result of above, the rdb_server will be run with unique application
ID and this ID will be passed via TDAQ_DB_NAME process environment variable to
all applications of this segment

---++ 3.6. Process Environment Calculation

The process environment of applications is calculated from the configuration
database. Each application may have own environment variables. The following
algorithm is used:
   * add setup and partition-specific variables: TDAQ_PARTITION,
   * TDAQ_IPC_INIT_REF and TDAQ_DB_PATH
   * add application's object environment (i.e. defined by
   * _ProcessEnvironment_ relationship for given object)
   * add environment of application's program object
   * for direct parent segment of application and parent segments of it's
   * segment (i.e. recursively from low level segment up to top level one
   * belonging to partition object):
      * add environment of segment object;
      * %N% add [[#3_5_7_Generation_of_Segment_Wide][segment-wide
      * environment]] defined by the infrastructure applications as defined by
      * the !SegmentProcEnvVar* variables);
   * add environment of partition object
   * for used software packages:
      * add software repositories
      * [[#3_5_5_Installation_Path_Variable][installation path variables]];
      * %N% extend environment variables via
      * [[#3_5_6_Extendable_Software_Packag][SW_PackageVariable objects]]
      * linked with the software package;
   * calculate TDAQ_DB value taking into account config plug-in technology
   * selected for partition object and running RDB infrastructure servers
   * calculate TDAQ_APPLICATION_OBJECT_ID (contains database object ID) and
   * TDAQ_APPLICATION_NAME (i.e. [[#3_3_1_Application_Names][unique
   * application name]]) variables

In above algorithm once a variable is defined on a high level, it's value will
be ignored from any lower levels. For example, TDAQ_PARTITION or
TDAQ_IPC_INIT_REF variables are defined automatically on the very top level
and cannot be changed by user even if are explicitly linked with application
object. Apart of this example, the environment defined for application has
higher priority, then defined for it's segment or partition. Similarly, the
environment variables defined by explicit parent segment have higher
priorities, than environment coming from top-level segments or partition.

The PATH and LD_LIBRARY_PATH variables are calculated for an application by
the [[DaqHltRunControl][RunControl]] taking into account the sw repositories
and external packages used by the application. It is not recommended to set
those values to an application by any mean.

The process environment of any application can be checked using dal_dump_apps
utility (to use it from command line it is necessary to define TDAQ_DB_DATA
variable often used by the partition object). Run this utility by passing OKS
xml file, partition name and optionally application ID, e.g. as shown below:
<verbatim>
  bash$ export TDAQ_DB_DATA=daq/partitions/be_test.data.xml
  bash$ dal_dump_apps -d oksconfig:$TDAQ_DB_DATA -p be_test -s -a
RootController
</verbatim>

---+ 4. Recommendations for Developers 

Below there are several guidelines for developers using the generated DAL. 

---++ 4.1. From what to start 

To access any configuration data one has to get the *configuration* object,
then find the *partition* object (that is the _entry point_ to get any another
objects relevant for the configuration) and to register the *converter* for
configuration parameters. The user's binary has to be linked with config and
dal libraries. If user is developing a plug-in or an application on top of the
[[DaqHltRunControl][RunControl]], !DataCollection (ac package), RCD or other
C++ frameworks, or Java-based [[DaqHltIGUI][IGUI]], then all previous steps
already done and the configuration and partition objects can be accessed via
API of above frameworks (see their documentation for more information). If
above is not the case, then above steps need to be done by the code of user's
application and they are described below: 

---+++ 4.1.1. How to create the configuration object 

To create the configuration object one has to call the constructor of the
*Configuration* class. The constructor has string parameter that identifies
which implementation plug-in will be used and what are parameters for this
plug-in. If the parameter of the constructor is empty, then the constructor
will use value of *TDAQ_DB* environment variable for it. When an application
is running by _setup_daq_, the *TDAQ_DB* variable is always created and set
into right value, so the user may leave the parameter of the constuctor empty.
In case of errors the constructor throws an exception. 

---++++!! C++ Example 

Read configuration from file _"daq/partitions/be_test.data.xml"_ using oks
plug-in. 

<verbatim>
    #include "oksdbinterfaces/Configuration.h"
    ...
    try {
      ::Configuration db("oksconfig:daq/partitions/be_test.data.xml");
      ... // user's code accessing configuration data
    }
    catch(dunedaq::oksdbinterfaces::Exception & ex) {
      std::cerr << "Caught config exception:\n" << ex << std::endl;
    }
</verbatim>

To use value of TDAQ_DB variable leave parameter of the constructor empty: 

<verbatim>
    ::Configuration db("");
</verbatim>

---++++!! Java Example 

Below there is an example reading data from rdb server with name _"RDB"_
running in the initial partition. 

<verbatim>
    import config.Configuration;
    ...
    try {
      config.Configuration db = new config.Configuration("rdbconfig:RDB");
    }
    catch (config.SystemException ex) {
      System.err.println( "Caught \'config.SystemException\':\n" +
ex.getMessage());
    }
</verbatim>

---+++ 4.1.2. Get partition object 

To get the partition object one has use DAL's algorithm *get_partition()*
defined for the *Partition* database class. The algorithm has string attribute
to find the partition object by ID. If it's value is empty, then value of
*TDAQ_PARTITION* variable is used. Similarly to the Configuration class, the
user may leave this parameter empty, if his application is running by
_setup_daq_ that always creates the variable with correct value. If the
partition object with such ID does not exist, then the _null_ is returned. 

---++++!! C++ Example 

Get partition object with ID _"be_test"_. 

<verbatim>
    #include "dal/Partition.h"
    #include <dal/util.h>
    ...
    ::Configuration db("oksconfig:daq/partitions/be_test.data.xml");
    if(const daq::core::Partition * p = daq::core::get_partition(db,
"be_test")) {
      ... // work with object p
    }
</verbatim>

---++++!! Java Example 

Get partition object with ID _"be_test"_. 
<verbatim>
    import config.Configuration;
    import dal.Partition;
    ...
    try {
      config.Configuration db = new config.Configuration("");
      dal.Partition p = dal.Algorithms.get_partition(db, "be_test");
      if(p != null) {
        ... // work with object p
    }
    catch (config.SystemException ex) {
      System.err.println( "Caught \'config.SystemException\':\n" +
ex.getMessage());
    }
</verbatim>

---+++ 4.1.3. Converter for substitution of variables

In some cases attributes can or shall have equal values or parts of them. For
example they can contain name of directory where software is installed, where
data are stored, where log files have to be written, etc. To avoid a necessity
to update several attributes synchronously and to related problems because of
inconsistent update, the configuration service supports the parameterization
via so-called *parameter* variables. If the value of a string attribute has to
be paramertized, then: 
   * the name of the parameter to be put inside braces and prefixed by dollar
   * sign, e.g. _"${LOGS_DIR}/my.log"_, where the LOGS_DIR is a parameter to
   * define directory for log files; if the LOGS_DIR="/tmp", the file name is
   * "/tmp/my.log"
   * the parameter to be created as an object of the *Variable* class and
   * linked with either partition or used segment object, e.g. for above
   * example on can to create variable with Name="LOGS_DIR" and Values="/tmp"
   * and to link it with user's segment via _"Parameters"_ relationship.

The mechanism of above mentioned strings parameters substitution is
implemented as generic configuration service attribute converter. Such
converter is not registered automatically, when the configuration object is
build by two main reasons: 
   * there are cases, when the automatic substitution shall not be done, e.g.
   * for database editors;
   * the parameters are partition and segments specific, so the configuration
   * object has no any special knowledge about them and what has to be done,
   * when those objects are updated.

To register the converter for strings attributes substitution the user has to
create the converter object and to register it on the configuration object,
e.g. as it is shown below. 

---++++!! C++ Example 

<verbatim>
    #include "oksdbinterfaces/Configuration.h"
    #include "dal/Partition.h"
    #include "dal/util.h"
    ...
    try {
      Configuration db;
      if(const daq::core::Partition * partition = daq::core::get_partition(db,
"be_test")) {
        db.register_converter(new daq::core::SubstituteVariables(db,
*partition));
      }
    }
    catch(dunedaq::oksdbinterfaces::Exception & ex) {
      std::cerr << "Caught config exception:\n" << ex << std::endl;
    }
</verbatim>

---++++!! Java Example 

<verbatim>
    import config.Configuration;
    import dal.Algorithm;
    import dal.SubstituteVariables;
    import dal.Partition;
    ...
    try {
      config.Configuration db = new config.Configuration("");
      dal.Partition p = dal.Algorithms.get_partition(db, "be_test");
      if(p != null) {
        db.register_converter(new dal.SubstituteVariables(db, p));
      }
    }
    catch (config.SystemException ex) {
      System.err.println( "Caught \'config.SystemException\':\n" +
ex.getMessage());
    }
</verbatim>

---+++ 4.1.4. Linkage 

For C++ it is enough to link against generated DAL and config libraries: 
<verbatim>
    -ldaq-core-dal -lconfig
</verbatim>
%ICON{note}% The implementation plug-ins like liboksconfig.so, librdbconfig.so
%and low level libraries used by them (e.g. liboks.so, librdb.so, etc.) are
%loaded dynamically and not needed to be linked with your binaries. It is
%enough, if they will be referenced by your LD_LIBRARY_PATH. 

For Java the dal.jar and config.jar files have to be in the CLASSPATH. The
implementation plug-ins like oksconfig.jar or rdbconfig.jar have to be in the
CLASSPATH as well. 

---++ 4.2. Applications

---+++ 4.2.1. Iterate all applications

There are two ways to get list of applications defined for the partition:
   * %Y% use get_all_applications() algorithm; if necessary, user can filter
   * out applications by type, segment and computer masks.
   * %ICON{choice-no}% go through tree of segments and calculate applications
   * for each of them; it is not recommended, since user has to know about
   * rules for normal and template applications, resources, resource sets and
   * their disabling, segment's infrastructure applications and controllers,
   * etc.

---++++ The get_all_applications algorithm

To get all applications defined for given partition one has to use
*Partition::get_all_applications()* algorithm, which is available for C++ and
Java DAL.

The C++ propotype is:
<verbatim>
    void daq::core::Partition::get_all_applications(
        std::vector<daq::core::AppConfig>& out, Configuration& db,
        std::set<std::string> * app_types = 0,
        std::set<const Segment *> * use_segments = 0,
        std::set<const Computer *> * use_hosts = 0
    ) const;
</verbatim>

where:
   * the _out_ parameter is the result of the algorithm's work (see
   * [[http://isscvs.cern.ch/cgi-bin/viewcvs-all.cgi/DAQ/online/dal/dal/app-config.h?root=atlastdaq&view=markup][dal/app-config.h]]
   * file or
   * [[http://pcatd12.cern.ch/doxygen/tdaq-01-08-04/html/classdaq_1_1core_1_1AppConfig.html][DoxyGen]]
   * for more information about *AppConfig* class to get application
   * parameters such as: application id, data collection node id, pointers to
   * application, segment and host configuration database objects);
   * the optional _app_types_ parameter defines set of application class names
   * (also takes into account their subclasses), which objects have to be
   * taken into account (use all applications, if the parameter is 0); 
   * the optional _use_segments_ parameter defines set of segments, which
   * applications have to be taken into account (use all segments, if the
   * parameter = 0); 
   * the optional _use_hosts_ parameter defines set of hosts where the
   * applications have to run on (use all hosts, if the parameter = 0). 

Above optional parameters can be combined.

The similar algorithm is defined for Java DAL:

<verbatim>
    dal.AppConfig[] dal.Partition.get_all_applications(
        String[] app_types,
        dal.Segment[] use_segments,
        dal.Computer[] use_hosts
    );
</verbatim>

The details of the *dal.AppConfig* can be found in the
[[http://isscvs.cern.ch/cgi-bin/viewcvs-all.cgi/DAQ/online/dal/jsrc/dal/AppConfig.java?root=atlastdaq&view=markup][dal/jsrc/dal/AppConfig.java]]
file.

#DalDumpAppConfigUtility To test get_all_applications() algorithm work there
is dedicated binary dal_dump_app_config (see also it's
[[http://isscvs.cern.ch/cgi-bin/viewcvs-all.cgi/DAQ/online/dal/examples/dal_dump_app_config.cpp?root=atlastdaq&view=markup][source]]
as example of code):

<verbatim>
    Usage: dal_dump_app_config [-d database-name] -p partition [-t [types
...]] [-c [ids ...]] [-s [ids ...]] 
    Options/Arguments: 
      --data | -d  database-name         name of the database (ignore TDAQ_DB
variable) 
      --partition-name | -p partition    name of the partition object 
      --application-types | -t types     filter out all applications except
given classes (and their subclasses) 
      --hosts | -c ids                   filter out all applications except
those which run on given hosts 
      --segments | -s ids                filter out all applications except
those which belong to given segments 
</verbatim>

Example below shows how to get all template and run control applications on
computer lxplus002 for a test partition (the Computer column is filtered out):

<verbatim>
    bash$ dal_dump_app_config -d
oksconfig:daq/partitions/lxplus_template_tests.data.xml \
      -p lxplus-test -c lxplus002.cern.ch -t TemplateApplication
RunControlApplication
    ===========================================================================================================
    | num | Application Object                      | Segment                |
MP:id | Application unique id  |
    ===========================================================================================================
    |   1 | test-L2@L2PUTemplateApplication         | lxplus01-19@HLTSegment |
32763 | L2PU-8187              |
    |   2 | test-L2@L2PUTemplateApplication         | lxplus01-19@HLTSegment |
32762 | L2PU-8186              |
    |   3 | test-L2@L2PUTemplateApplication         | lxplus01-19@HLTSegment |
32761 | L2PU-8185              |
    |   4 | test-L2@L2PUTemplateApplication         | lxplus01-19@HLTSegment |
32760 | L2PU-8184              |
    |   5 | rc@RunControlTemplateApplication        | lxplus01-19@HLTSegment |
0 | lxplus01-19:lxplus002  |
    |   6 | ExampleRC@RunControlTemplateApplication | ROSSegment-1@Segment   |
0 | ROSSegment-1:lxplus002 |
    |   7 | rc@RunControlTemplateApplication        | testSeg@Segment        |
0 | testSeg:lxplus002      |
    |   8 | RootController@RunControlApplication    | setup@OnlineSegment    |
0 | RootController         |
    |   9 | efd_controller@RunControlApplication    | efd_subfarm@EF_SubFarm |
0 | efd_controller         |
    |  10 | LVL2Segment-1@RunControlApplication     | LVL2Segment-1@Segment  |
0 | LVL2Segment-1          |
    ===========================================================================================================
</verbatim>

The *get_all_applications()* algorithm does not return applications which are
disabled in the scope of partition. However it returns applications for hosts,
which are off (i.e. the _State_ attribute has *false* value). This is done to
allow the !RunControl to inform user about error in case when host of a
critical application is off. If the user needs to skip the applications on
hosts switched off, then the code has to check state of the returned hosts,
e.g.:

<verbatim>
   Configuration db(...);
   daq::core::Partition * partition(...); 
   std::vector<daq::core::AppConfig> app_out;
   partition->get_all_applications(app_out, db);
   for(size_t i = 0; i < app_out.size(); ++i) {
     if (app_out[i].get_host().get_State() == false) {
       // do not take this application into account!
     }
     else {
       ...
     }
   }
</verbatim>

---++ 4.3. Segments and Resources 

---+++ 4.3.1. Build tree of segments and Graph of Resources

The simplest way to build tree of segments is to write recursive function
processing current segment and recursively invoking on nested segments. The
entry point of this function is the list of segments of the partition object.
The simplified fragment of such C++ code from
[[http://isscvs.cern.ch/cgi-bin/viewcvs-all.cgi/DAQ/online/dal/examples/dal_print_segments.cpp?root=atlastdaq&view=markup][dal_print_segments]]
example application is shown below:
<verbatim>
    static void print_segment(const daq::core::Segment& seg, unsigned int
recursion_level)
    {
      std::string s(recursion_level*4, ' ');
      std::cout << s << seg.UID() << std::endl;
      daq::core::SegmentIterator i = seg.get_Segments().begin();
      while(i != seg.get_Segments().end()) {
        print_segment(**i, recursion_level + 1); ++i;
      }
    }
    ...
    Configuration db;
    const daq::core::Partition * p = daq::core::get_partition(db, "");
    db.register_converter(new daq::core::SubstituteVariables(db, *p));
    daq::core::SegmentIterator i = p->get_Segments().begin();
    while(i != p->get_Segments().end()) {
      print_segment(**i, 0); ++i;
    }
</verbatim>

In a similar way for each segment it is possible to build graph of resources
using _Contains_ relationship defined for the !ResourceSet class and
_Resources_ relationship of the Segment class. %BR%
%X% Note, that a resource can belong to several segments or resource sets
%(e.g. a link resource belongs to ROS and ROD resource sets), so in general
%case the resources are linked into a graph and not to a single tree.

---+++ 4.3.2. Check disabled status

A segment or resource is disabled, when it or it's parent is added to the
partition's disabled relationship. In addition a resource can be implicitly
disabled when it is a resource set with _and_ or _or_ logic, or it's parent
belongs to such set.

To get disabled status of a resource or a segment the DAL algorithm
*disabled()* should be used. It is available for both C++ and Java DALs.

<!--***********************************************************--><!--Do NOT
remove the remaining lines, but add requested info as
appropriate--><!--***********************************************************-->

---
<!--For significant updates to the topic, consider adding your 'signature'
(beneath this editing box)--> *Major updates*:%BR% -- Main.isolov - 24 Aug
2007 

<!--Please add the name of someone who is responsible for this page so that
he/she can be contacted if changes are needed.
The creator's name will be added by default, but this can be replaced if
appropriate.
Put the name first, without dashes.-->%RESPONSIBLE% %REVINFO{"$wikiusername"
rev="1.1"}% %BR% <!--Once this page has been reviewed, please add the name and
the date e.g. Main.StephenHaywood - 31 Oct 2006 -->%REVIEW% *Never reviewed* 

%STOPINCLUDE% </img>
