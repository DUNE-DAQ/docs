# Getting Started

## Define information structure to be monitored 

Before using opmonlib it is important to understand and define what needs to be monitored.

Monotorable objects can then be captured in a `schema` file to create C++ structs using `ProtoBuf`. 
Documentation and instructions on generating schema data structures and using ProtoBuf can be found in the [ProtoBuf website](https://protobuf.dev/programming-guides/proto3/). 
Relevant pages include also the description of the C++ API.


% Examples of how to write schemas can be found [here](https://github.com/DUNE-DAQ/timing/tree/feature/op_mon/schema/timing). 
In general each `.protobuf` file contains definitions of blocks that are published as single units.
Each `schema` file will generate a C++ header file containing the structures which hold the monitoring data, as defined in the `.proto` file. 
Typically each module may only need one struct to hold its monitoring information; however it is possible to create multiple nested structs within a schema, which are filled by the same module.

% MR: add some example once listrev is done

### Valid types
As a generic schema language, `ProtoBuf` allows you do use simple types, but also lists, maps, etc.
Be aware that apart from basic types and nested messages, other quantities and are ignored.
ERS messages are generated whenever a structure with unpublishable entries are defined. 

## Filling and collecting structures
The `ProtoBuf` C++ API guide describes how to fill the structures you created. 
In order to publish the metric, the object has to be created from within an `MonitorableObject`, see [the header file](https://github.com/DUNE-DAQ/opmonlib/blob/mroda/protobuf/include/opmonlib/MonitorableObject.hpp).
In particular, a `DAQModule` *is* a `MonitorableObject`. 
Two main functions are relevant for publishing:
```C++
void publish( google::protobuf::Message &&,
     	      CustomOrigin && co = {},
              OpMonLevel l = to_level(EntryOpMonLevel::kDefault),
	      const element_id & element = "" ) const noexcept ;
virtual void generate_opmon_data(opmon_level) {return;}
```


* `publish` takes a ProtoBuf schmea object, it timestamps it with the time of the function call, it serializes it (syncronously) and publishes it (asyncronously) via one of the configured OpMonFacilities. This function can be called at anytime. 

* `generate_opmon_data` is a function which the monitoring system calls regularly (order of seconds). Its default behavious is null. Every user can freely imoplement this in their DAQModule in order to avoid setting up a thread to generate information regularly. Specific implementations are expected to call the `publish` function to actually create the metric.

### Details and good practices about the optional arguments 
Optional arguments of the `publish` function, allow you to


* specify a level of priority associated to the metric

* add additional custom information on the source of the metric, in the form of a `map<string, string>`, where the key is the type of the source, e.g. channel, and the second is the value, e.g. 4345. 

* extend the `opmon_id` of the caller `MonitorableObject` for the specific metric with more detailed information on the source of this metric. 

The `OpMonLevel` is a priority level designed to control the quantity of metrics generated by a tree is controllable. As a default, all messages are published. The lower the level, the highest the priority. 
They system can decide to disable entirely the metric publication regardless of the OpMonLevel. 
The system already provides some values to specify the `OpMonLevel` via an enum:
```C++
enum class EntryOpMonLevel : OpMonLevel {
    kTopPriority     = std::numeric_limits<OpMonLevel>::min(),
    kAsync           = std::numeric_limits<OpMonLevel>::max()/4,
    kDefault         = std::numeric_limits<OpMonLevel>::max()/2,
    kEventDriven     = (std::numeric_limits<OpMonLevel>::max()/4)*3,
    kLowestPrioriry  = std::numeric_limits<OpMonLevel>::max()-1
  };

```
but users are welcome to fill the gaps with whatever number they are happy to associate to their metric.

The usage of a custom origin is designed to provide information that is unrelated to software stack.
While the software stack might change (e.g, the name of an application or of a module can change because of configuration) some information like a crate number or a channel, are hardware related and they are independent of the software stack that provides this information. 
Examples of valid tags to be used in the custom origins are: server name, channel, links, etc. 
Adding information like application name, or session in the custorm origin is discouraged because it would be redundant. 

Using the specification of an additional element is the logical equivament of adding a child to the current `MonitorableObject` and generating the data from that child instead of the current object. 
This option is designed to be used for small objects that are simple or maybe short living such that they do not deserve to be full `MonitorableObjects`. 
While this is handy because it does not require the registration of sub `MonitorableObejcts`, this requires to check the uniqueness of the `opmon_id` associated to the metric every time a metric is public, and therefore it is less efficient. 

## Registering sub components for your DAQModule

In order to work correctly, each `MonitorableObject` has to be part of a monitoring tree, i.e. every `MonitorablreObject` has to be registered to the chain. 
This is done via the method
```C++
void register_child( std::string name, new_child_ptr ) ;
```

The metrics generated by the child will have an `opmon_id` in the form `parent_opmon_id.child_name`. 
The registration does now imply ownership of the child by the parent, as internally only weak pointers are utilised. 
If the child is destroyed, its pointer will evntually be removed from the chain. 

`DAQModule`s will be automatically registered by the application framework and developers have to write their code assuming that the module is registered in the monitoring tree from the moment of its creation. 
On the other hand, developers have to take care of the registration of subcomonents living inside their modules.



## Testing

The configuration of `opmonlib` is currently managed through the environment variables: `DUNEDAQ_OPMON_INTERVAL` and `DUNEDAQ_OPMON_LEVEL`. These can be seen further in `Application.cpp`:
```
  setenv("DUNEDAQ_OPMON_INTERVAL",    "10",0);
  setenv("DUNEDAQ_OPMON_LEVEL",  "1",0);
```
here `DUNEDAQ_OPMON_INTERVAL` sets the interval in seconds between each instance of calling `generate_opmon_data` (currently defautlting to 10 seconds), and `DUNEDAQ_OPMON_LEVEL` allows the user to define the level for `generatre_opmon_data` (currently set to 1). 



-----

<font size="1">
_Last git commit to the markdown source of this page:_


_Author: Marco Roda_

_Date: Fri Jul 12 16:10:31 2024 +0200_

_If you see a problem with the documentation on this page, please file an Issue at [https://github.com/DUNE-DAQ/opmonlib/issues](https://github.com/DUNE-DAQ/opmonlib/issues)_
</font>
